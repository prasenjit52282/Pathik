{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from library.readingData import read_data_from_folder\n",
    "from library.other_processing import get_processed_rows_for\n",
    "from library.map_processing import MapFeatExtractor\n",
    "from library.constants import over_write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder=\"/Users/smrlab/Desktop/TW_DP/TW_DP/Data/Two_W\"\n",
    "target_folder=\"/Users/smrlab/Desktop/TW_DP/TW_DP/Trails/TW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HELPERS\n",
    "\n",
    "mfe=MapFeatExtractor(data_folder) #map feature extractor\n",
    "\n",
    "folders=glob.glob(data_folder+\"/*/*\") #all folders in the data_folder\n",
    "\n",
    "#name output folders in this manner\n",
    "def get_folder_name(folder_number,src_folder_path):\n",
    "    return \"_\".join(src_folder_path.split(\"/\")[-3:])+f\"_{folder_number}\"\n",
    "\n",
    "#processing sensor & map data from a Lambda meter long patch\n",
    "def whole_data_from_raw_data_for_lambda_meter_patch_sensor_plus_map(data,Lambda=100): #default 100 meter\n",
    "    p_data=get_processed_rows_for(data,Lambda) #processing sensor_data\n",
    "    return mfe.add_map_features_to_processed_data(p_data,Lambda) #adding map data to it\n",
    "\n",
    "#check if any data file is not saved for a perticular folder\n",
    "def check_if_data_need_to_be_process(folder_name):\n",
    "    status=\\\n",
    "    (not os.path.exists(f\"./{folder_name}/DATA_100.csv\")) or \\\n",
    "    (not os.path.exists(f\"./{folder_name}/DATA_200.csv\")) or \\\n",
    "    (not os.path.exists(f\"./{folder_name}/DATA_300.csv\")) or \\\n",
    "    (not os.path.exists(f\"./{folder_name}/DATA_400.csv\")) or \\\n",
    "    (not os.path.exists(f\"./{folder_name}/DATA_500.csv\")) or \\\n",
    "    (not os.path.exists(f\"./{folder_name}/DATA_1000.csv\")) or \\\n",
    "    (not os.path.exists(f\"./{folder_name}/DATA_1500.csv\")) or \\\n",
    "    (not os.path.exists(f\"./{folder_name}/DATA_2000.csv\"))\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–‰                                        | 4/168 [03:49<2:22:37, 52.18s/it]"
     ]
    }
   ],
   "source": [
    "os.chdir(target_folder)\n",
    "\n",
    "folder_number=0\n",
    "\n",
    "for folder_path in tqdm(folders):\n",
    "    folder_name=get_folder_name(folder_number,folder_path)\n",
    "    if(check_if_data_need_to_be_process(folder_name) or over_write):\n",
    "        #either file doesnot exist or over-write is true then it will process!\n",
    "        os.makedirs(folder_name,exist_ok=True)\n",
    "        data=read_data_from_folder(folder_path)\n",
    "        #data.to_csv(f\"./{folder_name}/DATA.csv\",index=False)\n",
    "        whole_data_from_raw_data_for_lambda_meter_patch_sensor_plus_map(data,100).to_csv(f\"./{folder_name}/DATA_100.csv\",index=False)\n",
    "        whole_data_from_raw_data_for_lambda_meter_patch_sensor_plus_map(data,200).to_csv(f\"./{folder_name}/DATA_200.csv\",index=False)\n",
    "        whole_data_from_raw_data_for_lambda_meter_patch_sensor_plus_map(data,300).to_csv(f\"./{folder_name}/DATA_300.csv\",index=False)\n",
    "        whole_data_from_raw_data_for_lambda_meter_patch_sensor_plus_map(data,400).to_csv(f\"./{folder_name}/DATA_400.csv\",index=False)\n",
    "        whole_data_from_raw_data_for_lambda_meter_patch_sensor_plus_map(data,500).to_csv(f\"./{folder_name}/DATA_500.csv\",index=False)\n",
    "        whole_data_from_raw_data_for_lambda_meter_patch_sensor_plus_map(data,1000).to_csv(f\"./{folder_name}/DATA_1000.csv\",index=False)\n",
    "        whole_data_from_raw_data_for_lambda_meter_patch_sensor_plus_map(data,1500).to_csv(f\"./{folder_name}/DATA_1500.csv\",index=False)\n",
    "        whole_data_from_raw_data_for_lambda_meter_patch_sensor_plus_map(data,2000).to_csv(f\"./{folder_name}/DATA_2000.csv\",index=False)\n",
    "    \n",
    "    folder_number+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "class POI_open:\n",
    "    def __init__(self):\n",
    "        with open(\"/media/bittu/Data Storage/Projects/TW_DP/TW_DP/Data/global_dictionary.json\") as f:\n",
    "            self.open_poi_dict=json.load(f)\n",
    "    \n",
    "    def num_of_poi_open(self,time): #time in 24hrs format\n",
    "        num_poi=0\n",
    "        for k,l in self.open_poi_dict.items():\n",
    "            for ranges in l:\n",
    "                if time>=ranges[\"open\"] and time<=ranges[\"close\"]:\n",
    "                    num_poi+=1\n",
    "                    #print(k)\n",
    "        return num_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_cal=POI_open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_100=glob.glob(target_folder+\"/*/*_100.csv\")\n",
    "files_200=glob.glob(target_folder+\"/*/*_200.csv\")\n",
    "files_300=glob.glob(target_folder+\"/*/*_300.csv\")\n",
    "files_400=glob.glob(target_folder+\"/*/*_400.csv\")\n",
    "files_500=glob.glob(target_folder+\"/*/*_500.csv\")\n",
    "files_1000=glob.glob(target_folder+\"/*/*_1000.csv\")\n",
    "files_1500=glob.glob(target_folder+\"/*/*_1500.csv\")\n",
    "files_2000=glob.glob(target_folder+\"/*/*_2000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(i,fname):\n",
    "    df=pd.read_csv(fname)\n",
    "    df[\"Trail_type\"]= \"2018\" if \"2018\" in fname else \"2019\"\n",
    "    df['trail_no']=i\n",
    "    #other calculations\n",
    "    df['POI_open']=df.start_time.apply(lambda e:poi_cal.num_of_poi_open(e.split()[1][:-3]))\n",
    "    df['DayOfWeek']=df.start_time.apply(lambda e:pd.to_datetime(e,format=\"%m/%d/%Y %H:%M:%S\").dayofweek)\n",
    "    df['DayOfMonth']=df.start_time.apply(lambda e:pd.to_datetime(e,format=\"%m/%d/%Y %H:%M:%S\").day)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100=pd.concat([read_file(i,f) for i,f in enumerate(files_100)],axis=0).reset_index(drop=\"index\")\n",
    "df_200=pd.concat([read_file(i,f) for i,f in enumerate(files_200)],axis=0).reset_index(drop=\"index\")\n",
    "df_300=pd.concat([read_file(i,f) for i,f in enumerate(files_300)],axis=0).reset_index(drop=\"index\")\n",
    "df_400=pd.concat([read_file(i,f) for i,f in enumerate(files_400)],axis=0).reset_index(drop=\"index\")\n",
    "df_500=pd.concat([read_file(i,f) for i,f in enumerate(files_500)],axis=0).reset_index(drop=\"index\")\n",
    "df_1000=pd.concat([read_file(i,f) for i,f in enumerate(files_1000)],axis=0).reset_index(drop=\"index\")\n",
    "df_1500=pd.concat([read_file(i,f) for i,f in enumerate(files_1500)],axis=0).reset_index(drop=\"index\")\n",
    "df_2000=pd.concat([read_file(i,f) for i,f in enumerate(files_2000)],axis=0).reset_index(drop=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100.to_csv(target_folder+\"/processed_data_100.csv\",index=False)\n",
    "df_200.to_csv(target_folder+\"/processed_data_200.csv\",index=False)\n",
    "df_300.to_csv(target_folder+\"/processed_data_300.csv\",index=False)\n",
    "df_400.to_csv(target_folder+\"/processed_data_400.csv\",index=False)\n",
    "df_500.to_csv(target_folder+\"/processed_data_500.csv\",index=False)\n",
    "df_1000.to_csv(target_folder+\"/processed_data_1000.csv\",index=False)\n",
    "df_1500.to_csv(target_folder+\"/processed_data_1500.csv\",index=False)\n",
    "df_2000.to_csv(target_folder+\"/processed_data_2000.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot speed vs time\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
